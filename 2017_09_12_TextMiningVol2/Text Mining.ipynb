{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\n"
     ]
    }
   ],
   "source": [
    "print os.getcwd();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if necessary change the directory\n",
    "#os.chdir('c:\\\\Users\\..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"nightlife_sanfrancisco_en.csv\", header=0, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34086, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iexplore data set\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['company', 'text', 'stars'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great experience with Joey for a family karaoke night! He had every song that was requested by attendees. He literally had over 50,000+ songs to choose from -- everything from top 40s, R&B, hip hop, country, pop, and every song from any and every decade. He also played great dance music during transitions between singers.  \r\n",
      "\r\n",
      "He was extremely professional and accommodating throughout the entire planning process and during the actual event. We will definitely be using Joey again for future events - I highly recommend him for any of his services!\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words from \"words\"\n",
    "import nltk      # import stop words\n",
    "nltk.download('popular')  # Download text data sets, including stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "print stopwords.words(\"english\")\n",
    "#words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "#print words #  \"u\" before each word indicates that Python is internally representing each word as a unicode string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean all records\n",
    "def text_to_words( raw_text ):\n",
    "    # 1. Remove end of line\n",
    "    without_end_line = re.sub('\\n', ' ', raw_text)\n",
    "    # 2. Remove start of line\n",
    "    without_start_line = re.sub('\\r', ' ', without_end_line)\n",
    "    # 3. Remove punctuation\n",
    "    without_punctual = re.sub(ur'[\\W_]+',' ',without_start_line )\n",
    "    # 4. Replace number by XnumberX\n",
    "    without_number = re.sub('(\\d+\\s*)+', ' XnumberX ', without_punctual)\n",
    "    # 5. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", without_number) \n",
    "    # 6. Convert to lower case\n",
    "    lower_case = letters_only.lower()\n",
    "    # 7. Split into individual words\n",
    "    words = lower_case.split()\n",
    "    # 8. stemming  - algorithms Porter stemmer\n",
    "    meaningful_words = [ps.stem(word) for word in words]\n",
    "    # 9.Remove stop words \n",
    "    # Redundant step, removing later in Creating the bag of words step\n",
    "    #stops = set(stopwords.words(\"english\"))                  \n",
    "    #meaningful_words = [w for w in words if not w in stops]   \n",
    "    # 10. Join the words back into one string separated by space and return the result.\n",
    "    return( \" \".join( meaningful_words ))\n",
    "    #return (meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great experi with joey for a famili karaok night he had everi song that wa request by attende he liter had over xnumberx song to choos from everyth from top xnumberx s r b hip hop countri pop and everi song from ani and everi decad he also play great danc music dure transit between singer he wa extrem profession and accommod throughout the entir plan process and dure the actual event we will definit be use joey again for futur event i highli recommend him for ani of hi servic\n"
     ]
    }
   ],
   "source": [
    "clean_text = text_to_words( data[\"text\"][0] )\n",
    "print clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the data set text...\n",
      "\n",
      "Text 1000 of 34086\n",
      "\n",
      "Text 2000 of 34086\n",
      "\n",
      "Text 3000 of 34086\n",
      "\n",
      "Text 4000 of 34086\n",
      "\n",
      "Text 5000 of 34086\n",
      "\n",
      "Text 6000 of 34086\n",
      "\n",
      "Text 7000 of 34086\n",
      "\n",
      "Text 8000 of 34086\n",
      "\n",
      "Text 9000 of 34086\n",
      "\n",
      "Text 10000 of 34086\n",
      "\n",
      "Text 11000 of 34086\n",
      "\n",
      "Text 12000 of 34086\n",
      "\n",
      "Text 13000 of 34086\n",
      "\n",
      "Text 14000 of 34086\n",
      "\n",
      "Text 15000 of 34086\n",
      "\n",
      "Text 16000 of 34086\n",
      "\n",
      "Text 17000 of 34086\n",
      "\n",
      "Text 18000 of 34086\n",
      "\n",
      "Text 19000 of 34086\n",
      "\n",
      "Text 20000 of 34086\n",
      "\n",
      "Text 21000 of 34086\n",
      "\n",
      "Text 22000 of 34086\n",
      "\n",
      "Text 23000 of 34086\n",
      "\n",
      "Text 24000 of 34086\n",
      "\n",
      "Text 25000 of 34086\n",
      "\n",
      "Text 26000 of 34086\n",
      "\n",
      "Text 27000 of 34086\n",
      "\n",
      "Text 28000 of 34086\n",
      "\n",
      "Text 29000 of 34086\n",
      "\n",
      "Text 30000 of 34086\n",
      "\n",
      "Text 31000 of 34086\n",
      "\n",
      "Text 32000 of 34086\n",
      "\n",
      "Text 33000 of 34086\n",
      "\n",
      "Text 34000 of 34086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of text based on the dataframe column size\n",
    "num_text = data[\"text\"].size\n",
    "# Initialize an empty list to hold the clean text\n",
    "clean_data = []\n",
    "# Loop over each text; create an index i that goes from 0 to the length\n",
    "print \"Cleaning and parsing the data set text...\\n\"\n",
    "clean_data = []\n",
    "for i in xrange( 0, num_text ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print \"Text %d of %d\\n\" % ( i+1, num_text )                                                                    \n",
    "    clean_data.append( text_to_words( data[\"text\"][i] )) # in case of error run \"pip install -U nltk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great experience with Joey for a family karaoke night! He had every song that was requested by attendees. He literally had over 50,000+ songs to choose from -- everything from top 40s, R&B, hip hop, country, pop, and every song from any and every decade. He also played great dance music during transitions between singers.  \\r\\n\\r\\nHe was extremely professional and accommodating throughout the entire planning process and during the actual event. We will definitely be using Joey again for future events - I highly recommend him for any of his services!\\r\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare original and edited text\n",
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'great experi with joey for a famili karaok night he had everi song that wa request by attende he liter had over xnumberx song to choos from everyth from top xnumberx s r b hip hop countri pop and everi song from ani and everi decad he also play great danc music dure transit between singer he wa extrem profession and accommod throughout the entir plan process and dure the actual event we will definit be use joey again for futur event i highli recommend him for ani of hi servic'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Creating the bag of words...\\n\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = 'english',   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_data)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34086L, 5000L)\n"
     ]
    }
   ],
   "source": [
    "print train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "# Using in model, random forest example\n",
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, data[\"stars\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
